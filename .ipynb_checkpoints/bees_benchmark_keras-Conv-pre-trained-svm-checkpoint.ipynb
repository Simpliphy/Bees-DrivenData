{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the code is provided at [drivendata](http://blog.drivendata.org/2015/09/24/bees-benchmark/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm # smart progress bar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.misc import imread\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Convolution2D, Flatten, MaxPooling2D, Reshape, InputLayer,Conv2D\n",
    "from keras import backend as K\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34mbees\u001b[00m\r\n",
      "│   ├── \u001b[01;34mimages\u001b[00m\r\n",
      "│   │   ├── \u001b[01;34mtest\u001b[00m [992 entries exceeds filelimit, not opening dir]\r\n",
      "│   │   └── \u001b[01;34mtrain\u001b[00m [3969 entries exceeds filelimit, not opening dir]\r\n",
      "│   ├── SubmissionFormat.csv\r\n",
      "│   └── train_labels.csv\r\n",
      "├── bees_benchmark_keras-Conv.ipynb\r\n",
      "├── bees_benchmark_keras-Conv-pre-trained.ipynb\r\n",
      "├── bees_benchmark_keras-Conv-pre-trained-svm.ipynb\r\n",
      "├── bees_benchmark_keras.ipynb\r\n",
      "├── bees_benchmark_naive_bayes_classifer.ipynb\r\n",
      "├── bees-dot-tar\r\n",
      "├── \u001b[01;31mbees.tar.gz\u001b[00m\r\n",
      "└── raw_hog_daisy_rbf.csv\r\n",
      "\r\n",
      "4 directories, 10 files\r\n"
     ]
    }
   ],
   "source": [
    "# list the contents of the data directory\n",
    "!tree --filelimit=10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples is:  3969\n",
      "Predictions should be type: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>genus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>520</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3289</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2695</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4922</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  genus\n",
       "0   520    1.0\n",
       "1  3800    1.0\n",
       "2  3289    1.0\n",
       "3  2695    1.0\n",
       "4  4922    1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the labels using pandas\n",
    "labels = pd.read_csv(\"bees/train_labels.csv\")\n",
    "\n",
    "submission_format = pd.read_csv(\"bees/SubmissionFormat.csv\",\n",
    "                                index_col=0)\n",
    "\n",
    "print( \"Number of training examples is: \", labels.shape[0])\n",
    "print( \"Predictions should be type:\", labels.dtypes[0])\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.text.Text at 0x7f656a9a2a20>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEGCAYAAACJnEVTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65d0238978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.barplot(x=\"genus\", y=\"genus\", data=labels, estimator=lambda x: len(x) / len(labels) * 100)\n",
    "ax.set(ylabel=\"Percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000.jpg\r\n",
      "1002.jpg\r\n",
      "1003.jpg\r\n",
      "1004.jpg\r\n",
      "1007.jpg\r\n",
      "1008.jpg\r\n",
      "1009.jpg\r\n",
      "100.jpg\r\n",
      "1010.jpg\r\n",
      "1011.jpg\r\n",
      "ls: erreur d'écriture: Relais brisé (pipe)\r\n"
     ]
    }
   ],
   "source": [
    "# let's see what's in one of the image folders\n",
    "!ls bees/images/train/ | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels.sort_values(by='id',inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "\n",
    "temp = []\n",
    "skip_img = []\n",
    "\n",
    "#x = os.listdir(\"./bees/images/test/\")\n",
    "for img_number in labels.id.values :\n",
    "    \n",
    "    \n",
    "        image_path = os.path.join(\"./bees/images/train/\", str(img_number) + \".jpg\")\n",
    "        \n",
    "        try:\n",
    "            img = imread(image_path, mode = 'RGB')\n",
    "            #print(img_number)\n",
    "            img = img.astype('float32')\n",
    "            \n",
    "        except:\n",
    "            labels.drop(labels[labels.id == img_number].index, inplace=True)\n",
    "            continue\n",
    "            \n",
    "        if img.size != 120000 :\n",
    "            print(\"the id is {0} and the size is {1}\".format(img_number, img.size))\n",
    "            skip_img.append(img_number)\n",
    "            labels.drop(labels[labels.id == img_number].index, inplace=True)\n",
    "            continue\n",
    "        \n",
    "        temp.append(img)\n",
    "        \n",
    "\n",
    "training_images = np.stack(temp)\n",
    "training_images /= 255.0\n",
    "#training_images = np.swapaxes(training_images,axis1 = 1,axis2 =3)\n",
    "#training_images = np.swapaxes(training_images,axis1 = 2,axis2 =3)\n",
    "#training_images = training_images.reshape(-1, 120000).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_ = keras.utils.to_categorical(labels.genus.values, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3969, 200, 200, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3969, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louis/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# split into a local train/test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_images,\n",
    "                                                    labels_,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test,\n",
    "                                                    y_test,\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3175, 200, 200, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from scipy.misc import imread\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.misc import imresize\n",
    "from keras.models import Model\n",
    "\n",
    "img_rows, img_cols = 200, 200 # Resolution of inputs\n",
    "channel = 3\n",
    "num_classes = 2 \n",
    "\n",
    "\n",
    "#Get back the convolutional part of a VGG network trained on ImageNet\n",
    "base_model =  VGG16(weights='imagenet', include_top=False)\n",
    "base_model.summary()\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "#Create your own input format (here 3x200x200)\n",
    "input = Input(shape=(200,200,3),name = 'image_input')\n",
    "\n",
    "#Use the generated model \n",
    "output_vgg16_conv = base_model(input)\n",
    "\n",
    "#Create your own model \n",
    "\n",
    "\n",
    "#In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
    "base_model.summary()\n",
    "\n",
    "features_train = base_model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3175, 6, 6, 512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
